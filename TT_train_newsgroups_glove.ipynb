{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STGDoLbwaQwP"
   },
   "source": [
    "# Using pre-trained word embeddings\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2020/05/05<br>\n",
    "**Last modified:** 2020/05/05<br>\n",
    "**Description:** Text classification on the Newsgroup20 dataset using pre-trained GloVe word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKfVtmpcaQwR"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FAY49ON7aQwS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Only the TensorFlow backend supports string inputs.\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow.data as tf_data\n",
    "import keras\n",
    "from keras import layers\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI0MDhgFaQwS"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we show how to train a text classification model that uses pre-trained\n",
    "word embeddings.\n",
    "\n",
    "We'll work with the Newsgroup20 dataset, a set of 20,000 message board messages\n",
    "belonging to 20 different topic categories.\n",
    "\n",
    "For the pre-trained word embeddings, we'll use\n",
    "[GloVe embeddings](http://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uuQue2saQwT"
   },
   "source": [
    "## Download the Newsgroup20 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fjDhh4H7aQwT"
   },
   "outputs": [],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PC7h61BJaQwT"
   },
   "source": [
    "## Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5ziiWw41aQwT"
   },
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "# print(\"Number of directories:\", len(dirnames))\n",
    "# print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "# print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "# print(\"Some example filenames:\", fnames[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL0Tb97zaQwU"
   },
   "source": [
    "Here's a example of what one file contains:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0radKVCAaQwU"
   },
   "source": [
    "As you can see, there are header lines that are leaking the file's category, either\n",
    "explicitly (the first line is literally the category name), or implicitly, e.g. via the\n",
    "`Organization` filed. Let's get rid of the headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NbvHUd8eaQwU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alt.atheism, 1000 files found\n",
      "Processing comp.graphics, 1000 files found\n",
      "Processing comp.os.ms-windows.misc, 1000 files found\n",
      "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
      "Processing comp.sys.mac.hardware, 1000 files found\n",
      "Processing comp.windows.x, 1000 files found\n",
      "Processing misc.forsale, 1000 files found\n",
      "Processing rec.autos, 1000 files found\n",
      "Processing rec.motorcycles, 1000 files found\n",
      "Processing rec.sport.baseball, 1000 files found\n",
      "Processing rec.sport.hockey, 1000 files found\n",
      "Processing sci.crypt, 1000 files found\n",
      "Processing sci.electronics, 1000 files found\n",
      "Processing sci.med, 1000 files found\n",
      "Processing sci.space, 1000 files found\n",
      "Processing soc.religion.christian, 997 files found\n",
      "Processing talk.politics.guns, 1000 files found\n",
      "Processing talk.politics.mideast, 1000 files found\n",
      "Processing talk.politics.misc, 1000 files found\n",
      "Processing talk.religion.misc, 1000 files found\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "# print(\"Classes:\", class_names)\n",
    "# print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhnQQa8BaQwU"
   },
   "source": [
    "There's actually one category that doesn't have the expected number of files, but the\n",
    "difference is small enough that the problem remains a balanced classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFBGuFDqaQwU"
   },
   "source": [
    "## Shuffle and split the data into training & validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u539IrtZaQwU"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Extract a training & validation split\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDBTw5spaQwU"
   },
   "source": [
    "## Create a vocabulary index\n",
    "\n",
    "Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n",
    "Later, we'll use the same layer instance to vectorize the samples.\n",
    "\n",
    "Our layer will only consider the top 20,000 words, and will truncate or pad sequences to\n",
    "be actually 200 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ah2isQoMaQwU"
   },
   "outputs": [],
   "source": [
    "vectorizer = layers.TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf_data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjWEqZ_XaQwU"
   },
   "source": [
    "You can retrieve the computed vocabulary used via `vectorizer.get_vocabulary()`. Let's\n",
    "print the top 5 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lGwBTuJ5aQwV"
   },
   "outputs": [],
   "source": [
    "# vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrbLaXapaQwV"
   },
   "source": [
    "Let's vectorize a test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_Cn7ZkeKaQwV"
   },
   "outputs": [],
   "source": [
    "# output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "# output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4DBiKXJaQwV"
   },
   "source": [
    "As you can see, \"the\" gets represented as \"2\". Why not 0, given that \"the\" was the first\n",
    "word in the vocabulary? That's because index 0 is reserved for padding and index 1 is\n",
    "reserved for \"out of vocabulary\" tokens.\n",
    "\n",
    "Here's a dict mapping words to their indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oVCPTyDBaQwV"
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSfboHzcaQwV"
   },
   "source": [
    "As you can see, we obtain the same encoding as above for our test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fRCAjxxKaQwV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3457, 1682, 15, 2, 5776]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8DyoSCxaQwV"
   },
   "source": [
    "## Load pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2rw1px7aQwV"
   },
   "source": [
    "Let's download pre-trained GloVe embeddings (a 822M zip file).\n",
    "\n",
    "You'll need to run the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgFEtfRVaQwV"
   },
   "source": [
    "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gWDA77KaQwV"
   },
   "source": [
    "The archive contains text-encoded vectors of various sizes: 50-dimensional,\n",
    "100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n",
    "\n",
    "Let's make a dict mapping words (strings) to their NumPy vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46da29931e1483bb8e7f60539d34866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='GLOVE file:', options=(('50d', './glove.6B.50d.txt'), ('100d', './glove.6B.100d.txt'), (…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropdown = widgets.Dropdown(\n",
    "    options=[('50d', './glove.6B.50d.txt'), ('100d', './glove.6B.100d.txt'), ('200d', './glove.6B.200d.txt'), ('300d', './glove.6B.300d.txt')],\n",
    "    value='./glove.6B.50d.txt',\n",
    "    description='GLOVE file:',\n",
    ")\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "C8lSmPHDaQwV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = f\"{dropdown.value}\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrfGk_8KaQwV"
   },
   "source": [
    "Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n",
    "`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n",
    "vector for the word of index `i` in our `vectorizer`'s vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = path_to_glove_file.split(\".\")\n",
    "embedding_dim = 0\n",
    "for item in path_list:\n",
    "    if \"d\" in item:\n",
    "        embedding_dim = int(item[:-1])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "28WKpaGyaQwV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 18019 words (1981 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSLBiwh3aQwV"
   },
   "source": [
    "Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n",
    "\n",
    "Note that we set `trainable=False` so as to keep the embeddings fixed (we don't want to\n",
    "update them during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kXoQ7jVcaQwV"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    trainable=False,\n",
    ")\n",
    "embedding_layer.build((1,))\n",
    "embedding_layer.set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUgF9_gSaQwV"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "A simple 1D convnet with global max pooling and a classifier at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GrHzDfj1aQwW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 50)          1000100   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 128)         32128     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,215,416\n",
      "Trainable params: 215,316\n",
      "Non-trainable params: 1,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "besMROnhaQwW"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays\n",
    "are right-padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BDIOSN2caQwW"
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sePFLNfgaQwW"
   },
   "source": [
    "We use categorical crossentropy as our loss since we're doing softmax classification.\n",
    "Moreover, we use `sparse_categorical_crossentropy` since our labels are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AZmHz61haQwZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 15s 111ms/step - loss: 2.6263 - acc: 0.1513 - val_loss: 2.2205 - val_acc: 0.2348\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 1.9226 - acc: 0.3374 - val_loss: 1.6242 - val_acc: 0.4346\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 1.5836 - acc: 0.4559 - val_loss: 1.3471 - val_acc: 0.5449\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 11s 84ms/step - loss: 1.3702 - acc: 0.5305 - val_loss: 1.2470 - val_acc: 0.5661\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 10s 78ms/step - loss: 1.2102 - acc: 0.5848 - val_loss: 1.1763 - val_acc: 0.6019\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 11s 86ms/step - loss: 1.0930 - acc: 0.6256 - val_loss: 1.1313 - val_acc: 0.6142\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 8s 67ms/step - loss: 0.9857 - acc: 0.6628 - val_loss: 1.0992 - val_acc: 0.6219\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 0.8983 - acc: 0.6903 - val_loss: 1.0965 - val_acc: 0.6382\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 10s 83ms/step - loss: 0.7984 - acc: 0.7205 - val_loss: 1.0767 - val_acc: 0.6579\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 10s 76ms/step - loss: 0.7270 - acc: 0.7462 - val_loss: 1.0431 - val_acc: 0.6607\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 10s 81ms/step - loss: 0.6377 - acc: 0.7735 - val_loss: 1.1146 - val_acc: 0.6527\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 9s 75ms/step - loss: 0.5757 - acc: 0.7979 - val_loss: 1.2046 - val_acc: 0.6394\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.5173 - acc: 0.8168 - val_loss: 1.0658 - val_acc: 0.6824\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 9s 75ms/step - loss: 0.4507 - acc: 0.8418 - val_loss: 1.1686 - val_acc: 0.6664\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 10s 77ms/step - loss: 0.3990 - acc: 0.8588 - val_loss: 1.1919 - val_acc: 0.6734\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 10s 76ms/step - loss: 0.3657 - acc: 0.8727 - val_loss: 1.1812 - val_acc: 0.6792\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 8s 67ms/step - loss: 0.3205 - acc: 0.8898 - val_loss: 1.2912 - val_acc: 0.6747\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 8s 67ms/step - loss: 0.2845 - acc: 0.9037 - val_loss: 1.3923 - val_acc: 0.6659\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.2667 - acc: 0.9066 - val_loss: 1.3998 - val_acc: 0.6607\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 8s 65ms/step - loss: 0.2421 - acc: 0.9187 - val_loss: 1.4741 - val_acc: 0.6779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x246af2c2500>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: glove-newsgroups\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: glove-newsgroups\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"glove-newsgroups\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pretrained_word_embeddings",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
